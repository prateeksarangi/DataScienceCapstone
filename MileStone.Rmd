---
title: "Final Capstone Project Johns Hopkins University MileStone Report"
author: "Andrew Delos Santos"
date: "11/22/2019"
output: html_document
---

###Abstract

For this particular project, the goal is to develop a predictive text model from 
a corpus of text documents provided by Swiftkey, the partner for this particular
capstone. The project will go through the necessary procedures of having the data
cleaned along with exploratory data analysis for this stage of the project. The 
libraries that will help for this particular part of the project are the tm and 
ngram packages in order to conduct n-gram data and to use tm function features.
We first start with obtaining the data and cleaning it to make it cleaner.

###Getting and Cleaning the Data
I first loaded all the necessary packages and libraries. I then read the Twitter 
data in via a connection and from there converted to lower case and split the 
lines I needed to get rid of the punctuation. I also cleaned the data by removing 
all non-alphanumeric characters before and after a word and removed blank spaces.
Although there are two other text files that are provided in this capstone, I 
thought it was a good idea to first analyze the twitter text data. 

```{r echo = TRUE}
##Load libraries and packages
library(tm)
library(stringr)
library(dplyr)
library(ggplot2)
library(stopwords)
library(RColorBrewer)
library(wordcloud)
library(ngram)
library(stringr)
##Cleaning Data for the Twitter Data 
#Read in the Twitter file and open as read only to read the lines
#Close connection and make sure to split all punctuation 
twitter_file <- "final/en_US/en_US.twitter.txt"
twitter_connection <- file(twitter_file, "r")
twitter_lines <- readLines(twitter_connection, skipNul = T)
close(twitter_connection)
twitter_lines_lower <- tolower(twitter_lines)
twitter_lines_lower_updated <- unlist(strsplit(twitter_lines_lower, "[.,:;!?(){}<>]+"))
#Take data and replace all non-alphanumeric with space at beginning/end of a word
#Remove multiple spaces and spaces at the beginning and end of a line
#Remove multiple spaces and then remove spaces at the beginning and end of line
cleaned_twitter <- twitter_lines_lower_updated %>% 
  {gsub("^[^a-z0-9]+|[^a-z0-9]+$", " ", twitter_lines_lower_updated)} %>% 
  {gsub("[^a-z0-9]+\\s", " ", twitter_lines_lower_updated)} %>% 
  {gsub("\\s[^a-z0-9]+", " ", twitter_lines_lower_updated)} %>% 
  {gsub("\\s+", " ", twitter_lines_lower_updated)} %>% 
  {str_trim(twitter_lines_lower_updated)}
  head(cleaned_twitter, 20)
#Save in an rds file
saveRDS(cleaned_twitter, "cleaned_twitter.rds")
```

###Exploratory Data Analysis of N-gram models
 
##1-Gram Exploratory Analysis
I analyzed 1-gram models on the Twitter data and created exploratory graphs in 
order to get a better picture of what was happening. I also then analyzed both 
1-gram with and without stopwords. The findings were pretty interesting as a result.
From the data, it was not surprising to find that the was the most common word
with all stopwords and that just was the most common word without any stopwords. 
Also, it was very interesting to see that as more words increased, the count of
the top 250 words decreased. 

```{r echo = TRUE}
##1-Gram Exploratory Data Analysis with StopWords
#Create a table of one-gram words and create a data frame of the data to easily 
#pull elements from the data frame.
one_gram <- unlist(strsplit(cleaned_twitter, "\\s+"))
one_gram.frequency <- table(one_gram)
one_gram_df <- cbind.data.frame(names(one_gram.frequency), as.integer(one_gram.frequency))
names(one_gram_df) <- c("word", "frequency")
row.names(one_gram_df) <- one_gram_df[,1]
one_gram_df <- one_gram_df[order(-one_gram_df[["frequency"]]), ]
#Look at the first 20 1-gram words and create exploratory graphs/visualizations
head(one_gram_df, 20)
one_gram_df["i'm", ]
plot_one_gram <- ggplot(one_gram_df[1:20, ], aes(x = reorder(word, frequency), frequency)) + 
geom_col() + xlab(NULL) + coord_flip() + 
labs(title = "Twitter Top 20 Words (StopWords)", x = NULL, y = "Frequency")
plot_one_gram
plot(one_gram_df[1:500,][["frequency"]], xlab = "Top 500 Words", ylab = "Frequency")
one_gram_histogram <- ggplot(one_gram_df[1:250, ], aes(x = one_gram_df[1:250,][["frequency"]])) +
geom_histogram(colour = "black", fill = "red", bins = 50) + 
labs(title = "Twitter Top 250 Words", x = "Word Frequency", y = "Count")
one_gram_histogram
saveRDS(one_gram_df, "one_gram_df.rds")
#Data Cleaning No StopWords
#Create function to see stopwords. Subset and obtain words from the column 
#of the data frame and if it equals stop return the value then unlist the values
#and create a new data frame to then see the 1-gram words that are not stopwords.
f1 <- function(stop){
  return(which(one_gram_df[["word"]] == stop))
}
x1 <- unlist(lapply(stopwords("en"), f1))
one_gram_df_no_stop <- one_gram_df[-x1, ]
head(one_gram_df_no_stop, 20)
saveRDS(one_gram_df_no_stop, "one_gram_df_no_stop.rds")
#Exploratory Data Analysis No StopWords
ggplot(one_gram_df_no_stop[1:20, ], aes(x = reorder(word, frequency), frequency)) + 
geom_col() + 
xlab(NULL) + 
coord_flip() + 
labs(title = "Twitter Top 20 Words (No StopWords)", x = NULL, y = "Frequency")
#Generate Word Clouds for StopWords/No StopWords
set.seed(1234)
wordcloud(words = one_gram_df[["word"]], freq = one_gram_df[["frequency"]], min.freq = 1, max.words = 200, random.order = F, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
set.seed(1000)
wordcloud(words = one_gram_df_no_stop[["word"]], freq = one_gram_df_no_stop[["frequency"]], min.freq = 1, max.words = 50, random.order = F, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
```

##2-gram Exploratory Analysis
For the 2-gram exploratory analysis, I found it very interesting how the length
of the cleaned twitter lines was 5398325 lines long. Also, the top 2-gram phrase
with stopwords was "in the". 
```{r echo = TRUE}
##2-Gram Exploratory Data Analysis StopWords
#Repeat same steps as the 1-gram model.
cleaned_twitter[1]
twitter_final_two_gram <- cleaned_twitter[str_count(cleaned_twitter, "\\s+") > 0]
length(cleaned_twitter)
two_gram <- ngram(twitter_final_two_gram, n = 2)
two_gram_df <- get.phrasetable(two_gram)
head(two_gram_df, 20)
ngrams <- two_gram_df[["ngrams"]]
freq <- two_gram_df[["freq"]]
ggplot(two_gram_df[1:20, ], aes(x = reorder(ngrams, freq), freq)) + 
geom_col() + 
xlab(NULL) + 
coord_flip() + 
labs(title = "Twitter Top 20 2-Grams (stopwords)", x = NULL, y = "Frequency")
saveRDS(two_gram_df, "two_gram_df.rds")
```

##3-gram Exploratory Analysis
I then moved onto a 3-gram exploratory data analysis of the Twitter corpus. 
I foudn out that the most common 3-gram with stopwords phrase was "thanks for the".
```{r echo = TRUE}
##3-gram Exploratory Data Analysis StopWords
#Repeat the same steps again as the 1-gram and 2-gram models.
twitter_final_three_gram <- cleaned_twitter[str_count(cleaned_twitter, "\\s+") > 1]
length(cleaned_twitter)
three_gram <- ngram(twitter_final_three_gram, n = 3)
three_gram_df <- get.phrasetable(three_gram)
head(three_gram_df, 20)
ngrams <- three_gram_df[["ngrams"]]
freq <- three_gram_df[["freq"]]
ggplot(three_gram_df[1:20, ], aes(x = reorder(ngrams, freq), freq)) + 
geom_col() + xlab(NULL) + coord_flip() + 
labs(title = "Twitter Top 20 3-Grams (stopwords)", x = NULL, y = "Frequency")
saveRDS(three_gram_df, "three_gram_df.rds")
```

##2-gram and 3-gram Exploratory Analysis No Stopwords
After viewing all 1-gram, 2-gram, and 3-gram analysis with stopwords, I thought
it would be interesting to view the data with no stopwords. This particular part 
of the project took a long time as it was reading throughout the text to apply
the function I created to read for stopwords. 
```{r echo = TRUE}
#Create function to read for stopwords in english
twitter_function <- function(x){
 removeWords(x, stopwords("en"))
  }
final_twitter_lines <- unlist(lapply(cleaned_twitter, twitter_function))
final_twitter_lines <- str_trim(final_twitter_lines)
final_twitter_lines <- gsub("\\s+", " ", final_twitter_lines)
final_twitter_lines <- final_twitter_lines[nchar(final_twitter_lines) > 0]
length(final_twitter_lines)
##2-Gram No StopWords
twitter_final_lines_2 <- final_twitter_lines[str_count(final_twitter_lines, "\\s+") > 0]
length(twitter_final_lines_2)
two_gram_no_stop <- ngram(twitter_final_lines_2, n = 2)
two_gram_df_no_stop <- get.phrasetable(two_gram_no_stop)
head(two_gram_df_no_stop, 20)
ngrams <- two_gram_df_no_stop[["ngrams"]]
freq <- two_gram_df_no_stop[["freq"]]
ggplot(two_gram_df_no_stop[1:20, ], aes(x = reorder(ngrams, freq), freq)) + 
geom_col() + 
xlab(NULL) + 
coord_flip() + 
labs(title = "Twitter Top 20 2-Grams (No Stopwords)", x = NULL, y = "Frequency")
##3-Gram No StopWords
final_twitter_lines_3 <- final_twitter_lines[str_count(final_twitter_lines, "\\s+") > 1]
length(final_twitter_lines) 
three_gram_no_stop <- ngram(final_twitter_lines_3, n = 3)
three_gram_df_no_stop <- get.phrasetable(three_gram_no_stop)
head(three_gram_df_no_stop, 20)
ngrams <- three_gram_df_no_stop[["ngrams"]]
freq <- three_gram_df_no_stop[["freq"]]
ggplot(three_gram_df_no_stop[1:20, ], aes(x = reorder(ngrams, freq), freq)) + 
geom_col() + xlab(NULL) + coord_flip() + 
labs(title = "Twitter Top 20 3-Grams (No Stopwords)", x = NULL, y = "Frequency")
saveRDS(two_gram_df_no_stop, "2-gram_no_stop.rds")
saveRDS(three_gram_df_no_stop, "3-gram_no_stop.rds")
```

##Distinct Number of Words to cover the Text
```{r echo = TRUE}
#Use the 1-gram
head(one_gram_df, 20)
one_gram_df[["count"]] <- 1
one_gram_df[["count"]] <- cumsum(one_gram_df[["count"]])
one_gram_df[["coverage"]] <- cumsum(one_gram_df[["frequency"]]) / sum(one_gram_df[["frequency"]]) * 100
one_gram_df_cover <- one_gram_df[one_gram_df[["coverage"]] <= 91,]
head(one_gram_df_cover, 5)
data_points <- rbind(tail(one_gram_df_cover[one_gram_df_cover[["coverage"]] <= 50, ], 1), tail(one_gram_df_cover[one_gram_df_cover[["coverage"]] <= 90, ], 1))
plot(one_gram_df_cover[["coverage"]], main = "Number of Words Covering Twitter Text File", xlab = "# of Words", ylab = "% of Covered Twitter Text")
saveRDS(one_gram_df_cover, "one_gram_df_cover.rds")
```

##1-Gram US News and US Blogs with StopWords Cleaning and Exploratory Analysis
From the findings, they were very similar to that of the Twitter text data which 
should be the case. 
```{r echo = TRUE}
#US News Cleaning Data
#Repeat similar steps as Twitter text data except now for other two texts
us_news <- file("final/en_US/en_US.news.txt", "r")
us_news_lines <- readLines(us_news, skipNul = T)
close(us_news)
lower_us_news <- tolower(us_news_lines)
lower_us_news_updated <- unlist(strsplit(lower_us_news, "[.,:;!?(){}<>]+"))
cleaned_us_news <- lower_us_news_updated %>% 
  {gsub("^[^a-z0-9]+|[^a-z0-9]+$", " ", lower_us_news_updated)} %>% 
  {gsub("[^a-z0-9]+\\s", " ", lower_us_news_updated)} %>% 
  {gsub("\\s[^a-z0-9]+", " ", lower_us_news_updated)} %>% 
  {gsub("\\s+", " ", lower_us_news_updated)} %>% 
  {str_trim(lower_us_news_updated)}
  head(cleaned_us_news, 20)
saveRDS(cleaned_us_news, "cleaned_us_news.rds")
#US News 1-Gram 
one_gram_us_news <- unlist(strsplit(cleaned_us_news, "\\s+"))
one_gram.frequency_news <- table(one_gram_us_news)
one_gram_df_news <- cbind.data.frame(names(one_gram.frequency_news), as.integer(one_gram.frequency_news))
names(one_gram_df_news) <- c("words", "freq")
row.names(one_gram_df_news) <- one_gram_df_news[,1]
one_gram_df_news <- one_gram_df_news[order(-one_gram_df_news[["freq"]]), ]
head(one_gram_df_news, 20)
one_gram_df_news["i'm", ]
plot_one_gram_df_news <- ggplot(one_gram_df_news[1:20, ], aes(x = reorder(words, freq), freq)) + 
geom_col() + xlab(NULL) + coord_flip() + 
labs(title = "US News Top 20 Words (StopWords)", x = NULL, y = "Frequency")
plot_one_gram_df_news
plot(one_gram_df_news[1:500,][["freq"]], xlab = "Top 500 Words", ylab = "Frequency")
one_gram_histogram_news <- ggplot(one_gram_df_news[1:250, ], aes(x = one_gram_df_news[1:250,][["freq"]])) + 
geom_histogram(colour = "black", fill = "red", bins = 50) + 
labs(title = "US News Top 250 Words", x = "Word Frequency", y = "Count")
one_gram_histogram_news
set.seed(1234)
wordcloud(words = one_gram_df_news[["words"]], freq = one_gram_df_news[["freq"]], min.freq = 1, max.words = 200, random.order = F, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
saveRDS(one_gram_df_news, "US_News_One_Gram.rds")
#US Blogs Cleaning Data
us_blogs <- file("final/en_US/en_US.blogs.txt", "r")
us_blogs_lines <- readLines(us_blogs, skipNul = T)
close(us_blogs)
lower_us_blogs <- tolower(us_blogs)
lower_us_blogs_updated <- unlist(strsplit(lower_us_news, "[.,:;!?(){}<>]+"))
cleaned_us_blogs <- lower_us_blogs_updated %>% 
  {gsub("^[^a-z0-9]+|[^a-z0-9]+$", " ", lower_us_blogs_updated)} %>% 
  {gsub("[^a-z0-9]+\\s", " ", lower_us_blogs_updated)} %>% 
  {gsub("\\s[^a-z0-9]+", " ", lower_us_blogs_updated)} %>% 
  {gsub("\\s+", " ", lower_us_blogs_updated)} %>% 
  {str_trim(lower_us_blogs_updated)}
  head(cleaned_us_blogs, 20)
saveRDS(cleaned_us_blogs, "cleaned_us_blogs.rds")
#US Blogs 1-Gram
one_gram_us_blogs <- unlist(strsplit(cleaned_us_blogs, "\\s+"))
one_gram.frequency_blogs <- table(one_gram_us_blogs)
one_gram_df_blogs <- cbind.data.frame(names(one_gram.frequency_blogs), as.integer(one_gram.frequency_blogs))
names(one_gram_df_blogs) <- c("wrds", "frq")
row.names(one_gram_df_blogs) <- one_gram_df_blogs[,1]
one_gram_df_blogs <- one_gram_df_blogs[order(-one_gram_df_blogs[["frq"]]), ]
head(one_gram_df_blogs, 20)
one_gram_df_blogs["i'm", ]
plot_one_gram_df_blogs <- ggplot(one_gram_df_blogs[1:20, ], aes(x = reorder(wrds, frq), frq)) + 
geom_col() + 
xlab(NULL) + 
coord_flip() + 
labs(title = "US Blogs Top 20 Words (StopWords)", x = NULL, y = "Frequency")
plot_one_gram_df_blogs
plot(one_gram_df_blogs[1:500,][["frq"]], xlab = "Top 500 Words", ylab = "Frequency")
one_gram_histogram_blogs <- ggplot(one_gram_df_blogs[1:250, ], aes(x = one_gram_df_blogs[1:250,][["frq"]])) + 
geom_histogram(colour = "black", fill = "red", bins = 50) + 
labs(title = "US Blogs Top 250 Words", x = "Word Frequency", y = "Count")
one_gram_histogram_news
set.seed(1234)
wordcloud(words = one_gram_df_blogs[["wrds"]], freq = one_gram_df_blogs[["frq"]], min.freq = 1, max.words = 200, random.order = F, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
saveRDS(one_gram_df_blogs, "US_Blogs_One_Gram.rds")
```

##RDS File References 
```{r echo = TRUE}
saveRDS(cleaned_twitter, "cleaned_twitter.rds")
saveRDS(one_gram_df, "one_gram_df.rds")
saveRDS(one_gram_df_no_stop, "one_gram_df_no_stop.rds")
saveRDS(two_gram_df, "two_gram_df.rds")
saveRDS(three_gram_df, "three_gram_df.rds")
saveRDS(two_gram_df_no_stop, "2-gram_no_stop.rds")
saveRDS(three_gram_df_no_stop, "3-gram_no_stop.rds")
saveRDS(one_gram_df_cover, "one_gram_df_cover.rds")
saveRDS(cleaned_us_news, "cleaned_us_news.rds")
saveRDS(one_gram_df_news, "US_News_One_Gram.rds")
saveRDS(cleaned_us_blogs, "cleaned_us_blogs.rds")
saveRDS(one_gram_df_blogs, "US_Blogs_One_Gram.rds")
```
